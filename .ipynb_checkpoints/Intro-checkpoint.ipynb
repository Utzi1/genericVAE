{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6207893-a2b8-41fb-b9b9-175f692dcbd5",
   "metadata": {},
   "source": [
    "# Intro to the usage of the genericVAE\n",
    "## Requirements\n",
    "\n",
    "This notebook uses python >= 3.7 with tensorflow version 2.12.0, numpy version 1.23.5, pandas version 2.0.0 and h5py 3.8.0.\n",
    "\n",
    "## Data\n",
    "\n",
    "The GTEx V8 study serves as a brilliant dataset for demonstration.\n",
    "At first we want to download a set that unifies all gene read counts, for different types of cells, in the study.\n",
    "\n",
    "The inclusion and exclusion criteria were specified as:\n",
    "\n",
    "1. 21 ≤ Age (years) ≤ 70\n",
    "2. 18.5 < Body Mass Index < 35\n",
    "3. Time between death and tissue collection less than 24 hours\n",
    "4. No whole blood transfusion within 48 hours prior to death\n",
    "5. No history of metastatic cancer\n",
    "6. No chemotherapy or radiation therapy within the 2 years prior to death\n",
    "7. Generally unselected for presence or absence of diseases or disorders, except for potentially communicable diseases that disqualify someone to donate organs or tissues would also be disqualifying for GTEx.\n",
    "\n",
    "This data can be downloaded from the following URL: `https://storage.googleapis.com/gtex_analysis_v8/rna_seq_data/GTEx_Analysis_2017-06-05_v8_RNASeQCv1.1.9_gene_reads.gct.gz` .\n",
    "After the download, move it to a folder `genericVAE/data` where you'll place all other data, too.\n",
    "To read the data a helping function is provided:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4eb9b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import reading\n",
    "data = reading.read_gct_from_GTEx(\"data/GTEx_Analysis_2017-06-05_v8_RNASeQCv1.1.9_gene_reads.gct.gz\")\n",
    "# data.info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11b2cae",
   "metadata": {},
   "source": [
    "We see, that there is rows specifying genes (by entrez-id and the HUGO-symbol) and columns specifying samples.\n",
    "As the dataset is quite a chunk, for further analysis we want to safe this data in format, that's faster accessible, for us that's h5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30eb10d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_hdf(\"data/GTEx_Analysis_2017-06-05_v8_RNASeQCv1.1.9_gene_reads.h5\", key=\"data\")\n",
    "del data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df3954c",
   "metadata": {},
   "source": [
    "Next we want to filter the dataset for genes, that are relevant for our current scope.\n",
    "For the beginning, lets focus on genes, that are part of the STRING database.\n",
    "For this purpose there is a list provided, named `h_S_string.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b6d9cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25a4b07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_hdf(\"data/GTEx_Analysis_2017-06-05_v8_RNASeQCv1.1.9_gene_reads.h5\", key = \"data\")\n",
    "in_string = pd.read_csv(\"h_S_string.txt\", sep=\"\\t\")[\"preferred_name\"]\n",
    "# filter the frame\n",
    "fdat = data.loc[data[\"Description\"].isin(in_string)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8b6328",
   "metadata": {},
   "source": [
    "Now we have a subset of 17383 samples reduced to 18647 gene reads per sample.\n",
    "Prior to training there needs to be done some pre-processing.\n",
    "Experimentation has shown, that log-transformation and scaling is enough.\n",
    "For the sake of performance we will switch to `numpy` and later reconstruct the data-frame structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fba55a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import helpers.data_preprocessing as prep\n",
    "\n",
    "# for the log-transformation\n",
    "logdat = prep.log_norm(np.array(fdat)[:, 2:])\n",
    "# for the scaling, we scale sample-wise, here we add 1 to avoid zero-division:\n",
    "sdat = prep.scale_by_sample(logdat+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5470f09",
   "metadata": {},
   "source": [
    "## Build and fit a model\n",
    "\n",
    "Now we can build a variational autoencoder (VAE) model to be trained with the data we just prepared.\n",
    "At first we specify the parameters the model will be based on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa5f1553",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-17 13:36:18.764596: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-17 13:36:18.797184: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-17 13:36:18.797927: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-17 13:36:19.484279: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import class_definitions.generic_VAE as gvae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34898389",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-17 13:36:20.066104: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "# as we train sample-wise, we have to transpose the dataframe\n",
    "input_shape = sdat.T.shape[1]\n",
    "\n",
    "vae = gvae.Builder(input_shape=input_shape,\n",
    "                   encoder_shape=[1000],\n",
    "                   decoder_shape=[1000],\n",
    "                   latent_dims=50)\n",
    "\n",
    "vae = gvae.VAE(vae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fa72e4",
   "metadata": {},
   "source": [
    "Next we compile it, in concordance with the `keras` functional API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a0375d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d588ce1",
   "metadata": {},
   "source": [
    "Before we can start training, lets find a batch-size assuming we want to train in 100 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3fb969e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = sdat.shape[1]//100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaa0699",
   "metadata": {},
   "source": [
    "And fit it to our pre-processed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11627ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "101/101 [==============================] - 25s 238ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 2/100\n",
      "101/101 [==============================] - 24s 233ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 3/100\n",
      "101/101 [==============================] - 24s 233ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 4/100\n",
      "101/101 [==============================] - 24s 233ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 5/100\n",
      "101/101 [==============================] - 24s 233ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 6/100\n",
      "101/101 [==============================] - 23s 233ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 7/100\n",
      "101/101 [==============================] - 23s 233ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 8/100\n",
      "101/101 [==============================] - 24s 233ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 9/100\n",
      "101/101 [==============================] - 24s 233ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 10/100\n",
      "101/101 [==============================] - 24s 233ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 11/100\n",
      "101/101 [==============================] - 23s 232ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 12/100\n",
      "101/101 [==============================] - 23s 232ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 13/100\n",
      "101/101 [==============================] - 24s 233ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 14/100\n",
      "101/101 [==============================] - 24s 233ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 15/100\n",
      "101/101 [==============================] - 23s 232ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 16/100\n",
      "101/101 [==============================] - 23s 232ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 17/100\n",
      "101/101 [==============================] - 24s 233ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 18/100\n",
      "101/101 [==============================] - 23s 232ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 19/100\n",
      "101/101 [==============================] - 24s 233ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 20/100\n",
      "101/101 [==============================] - 23s 232ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 21/100\n",
      "101/101 [==============================] - 23s 232ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 22/100\n",
      "101/101 [==============================] - 23s 232ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 23/100\n",
      "101/101 [==============================] - 23s 232ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 24/100\n",
      "101/101 [==============================] - 24s 233ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 25/100\n",
      "101/101 [==============================] - 24s 233ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 26/100\n",
      "101/101 [==============================] - 24s 233ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 27/100\n",
      "101/101 [==============================] - 23s 232ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 28/100\n",
      "101/101 [==============================] - 23s 232ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 29/100\n",
      "101/101 [==============================] - 23s 232ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 30/100\n",
      "101/101 [==============================] - 23s 232ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 31/100\n",
      "101/101 [==============================] - 23s 232ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 32/100\n",
      "101/101 [==============================] - 23s 232ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 33/100\n",
      "101/101 [==============================] - 23s 232ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 34/100\n",
      "101/101 [==============================] - 23s 232ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 35/100\n",
      "101/101 [==============================] - 23s 232ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 36/100\n",
      "101/101 [==============================] - 23s 232ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 37/100\n",
      "101/101 [==============================] - 23s 232ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 38/100\n",
      "101/101 [==============================] - 23s 232ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 39/100\n",
      "101/101 [==============================] - 23s 232ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 40/100\n",
      "101/101 [==============================] - 24s 233ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 41/100\n",
      "101/101 [==============================] - 23s 232ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 42/100\n",
      "101/101 [==============================] - 24s 233ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 43/100\n",
      "101/101 [==============================] - 23s 233ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 44/100\n",
      "101/101 [==============================] - 24s 233ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 45/100\n",
      "101/101 [==============================] - 23s 232ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 46/100\n",
      "101/101 [==============================] - 23s 232ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 47/100\n",
      "101/101 [==============================] - 23s 232ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 48/100\n",
      "101/101 [==============================] - 23s 233ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 49/100\n",
      "101/101 [==============================] - 23s 232ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 50/100\n",
      "101/101 [==============================] - 23s 233ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 51/100\n",
      "101/101 [==============================] - 23s 232ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 52/100\n",
      "101/101 [==============================] - 24s 233ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 53/100\n",
      "101/101 [==============================] - 24s 233ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 54/100\n",
      "101/101 [==============================] - 23s 232ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 55/100\n",
      "101/101 [==============================] - 24s 237ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 56/100\n",
      "101/101 [==============================] - 25s 251ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 57/100\n",
      "101/101 [==============================] - 23s 230ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 58/100\n",
      "101/101 [==============================] - 24s 240ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 59/100\n",
      "101/101 [==============================] - 24s 240ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 60/100\n",
      "101/101 [==============================] - 24s 235ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 61/100\n",
      "101/101 [==============================] - 24s 235ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 62/100\n",
      "101/101 [==============================] - 24s 235ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 63/100\n",
      "101/101 [==============================] - 24s 234ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 64/100\n",
      "101/101 [==============================] - 24s 237ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 65/100\n",
      "101/101 [==============================] - 24s 235ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 66/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101/101 [==============================] - 24s 235ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 67/100\n",
      "101/101 [==============================] - 24s 236ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 68/100\n",
      "101/101 [==============================] - 24s 236ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 69/100\n",
      "101/101 [==============================] - 24s 242ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 70/100\n",
      "101/101 [==============================] - 24s 237ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 71/100\n",
      "101/101 [==============================] - 24s 233ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 72/100\n",
      "101/101 [==============================] - 24s 234ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 73/100\n",
      "101/101 [==============================] - 24s 237ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 74/100\n",
      "101/101 [==============================] - 24s 242ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 75/100\n",
      "101/101 [==============================] - 24s 240ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 76/100\n",
      "101/101 [==============================] - 28s 281ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 77/100\n",
      "101/101 [==============================] - 24s 237ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 78/100\n",
      "101/101 [==============================] - 25s 244ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 79/100\n",
      " 10/101 [=>............................] - ETA: 21s - loss: nan - reconstruction_loss: nan - kl_loss: nan"
     ]
    }
   ],
   "source": [
    "vae.fit(sdat.T, epochs=100, batch_size= batchsize,workers=64, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85048b5a",
   "metadata": {},
   "source": [
    "In this case we just trained once with each sample.\n",
    "So far no training best practice was established.\n",
    "\n",
    "The difference here to usual cases of generative frameworks is, that the generative process is not the thing we aim to optimize, here the aim is to produce a representative latent space.\n",
    "\n",
    "This difference could also allow the user to iterate several times over the same data and cause soft overfitting as we don't aim to create an intelligent model.\n",
    "In case of using such a mechanism the introduction of drop-outs however is a good idea to still suppress overfitting effects getting too large.\n",
    "\n",
    "In the end, the user has to set the hyper parameters in a way, that seems appropriate for the underlying case.\n",
    "\n",
    "After fitting of the model one want's to extract the reconstruction errors.\n",
    "These reconstruction errors are the basis for the further analysis.\n",
    "The variational autoencoder reduces the input over our hidden layer towards\n",
    "a representation of only 18 dimensions.\n",
    "Therefor the system is ought to train itself in a way, that allows latent conservation of features that help to reduce this error.\n",
    "Some features will contribute more to this latent representation, some contribute less.\n",
    "Based on the contribution of a feature to the latent representation it's reconstruction error\n",
    "will be smaller (high contribution) or greater (less contribution).\n",
    "For this reason we'll remap the gene-names to the reconstruction errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcddd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to extract the reconstruction-errors we simply access the vae-obj\n",
    "recons = vae.fwise_recon_error_tracker.result().numpy()\n",
    "recon_frame = pd.DataFrame({\"name\" : fdat[\"Description\"], \"recon_1\" : recons})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196ef56a",
   "metadata": {},
   "source": [
    "## Analysis of the reconstruction error list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da50c13",
   "metadata": {},
   "source": [
    "To dig a little deeper we use R.\n",
    "To use R inside of this notebook we use the ipython rpy2 extension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5657980",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a4592d",
   "metadata": {},
   "source": [
    "To invoke R in an IPythbon cell simply use the magic function `%%R`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf108d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R \n",
    "string = paste(\"Hello, it's R \", R.Version()$major, R.Version()$minor, \" printing this\")\n",
    "print(string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f147786f",
   "metadata": {},
   "source": [
    "### Enrichment analysis\n",
    "\n",
    "At first let's have a look the top-200 genes with the lowest reconstruction error and check the enriched terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dab5529",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library('tidyverse')\n",
    "library('clusterProfiler')\n",
    "library('org.Hs.eg.db')\n",
    "library('tidyverse')\n",
    "library('STRINGdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddf17fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i recon_frame\n",
    "\n",
    "data = recon_frame %>% filter(recon_1 != 0)\n",
    "\n",
    "ref = data$name\n",
    "\n",
    "top_500 = data$name[1:501]\n",
    "\n",
    "# for cellular component\n",
    "ego = enrichGO(gene = top_500,\n",
    "               OrgDb = org.Hs.eg.db,\n",
    "               universe = ref,\n",
    "               ont = \"CC\",\n",
    "               keyType = \"SYMBOL\")\n",
    "\n",
    "barplot(ego)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a840b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# and molecular function\n",
    "ego = enrichGO(gene = top_500,\n",
    "               OrgDb = org.Hs.eg.db,\n",
    "               universe = ref,\n",
    "               ont = \"MF\",\n",
    "               keyType = \"SYMBOL\")\n",
    "\n",
    "barplot(ego)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c061778",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# and biological process:\n",
    "ego = enrichGO(gene = top_500,\n",
    "               OrgDb = org.Hs.eg.db,\n",
    "               universe = ref,\n",
    "               ont = \"BP\",\n",
    "               keyType = \"SYMBOL\")\n",
    "\n",
    "barplot(ego)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e93ef5",
   "metadata": {},
   "source": [
    "### Network analysis\n",
    "\n",
    "We can also look closer at the genes by plotting them as PPI network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea69786",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "string_db = STRINGdb$new(species=9606,\n",
    "                         score_threshold=200,\n",
    "                         input_directory=\"\")\n",
    "\n",
    "top_500 = data.frame(\"gene\"=top_500)\n",
    "\n",
    "top_500_mapped = string_db$map(top_500, \"gene\", removeUnmappedRows = T)\n",
    "string_db$plot_network(top_500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603d912c",
   "metadata": {},
   "source": [
    "Seems like we have a few heavy interconnected regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04029377",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "ntwrk <- string_db$get_subnetwork(top_500_mapped$STRING_id)\n",
    "\n",
    "library('igraph')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275178d2",
   "metadata": {},
   "source": [
    "With igraph, let us have a look at some measures we can easily compute on the extracted network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165ec24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# \n",
    "order(betweenness(ntwrk))\n",
    "mean_distance(ntwrk)\n",
    "clique.number(ntwrk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402d0091",
   "metadata": {},
   "source": [
    "For a further analysis let's switch to the package `BioNAR`.\n",
    "Let's have a look at how well our subnetwork follows the power-law:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5aa70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library('BioNAR')\n",
    "clusters <- calcAllClustering(ntwrk)\n",
    "pFit <- fitDegree( as.vector(igraph::degree(graph=clusters)),threads=1, Nsim=5,\n",
    "\n",
    "                    plot=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06511607",
   "metadata": {},
   "source": [
    "The package also allows us to identify clusters in the extracted list.\n",
    "These clusters then can get extracted and be plotted clusterwise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57f8a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "alg = \"louvain\"\n",
    "library(\"randomcoloR\")\n",
    "\n",
    "clsuters <- calcCentrality(ntwrk)\n",
    "getCentralityMatrix(clusters)\n",
    "clusters <- calcClustering(clsuters, alg)\n",
    "summary(clusters)\n",
    "V(clusters)$louvain\n",
    "\n",
    "mem_df <- data.frame(names=V(clusters)$name,membership=as.numeric(V(clusters)$louvain))\n",
    "\n",
    "palette <- distinctColorPalette(max(as.numeric(mem_df$membership)))\n",
    "\n",
    "lay <- layoutByCluster(clusters, mem_df,layout = layout_nicely)\n",
    "\n",
    "plot(clusters,vertex.size=3,layout=lay,\n",
    "        vertex.label=NA,\n",
    "        vertex.color=palette[as.numeric(mem_df$membership)],\n",
    "        edge.color='grey95')\n",
    "\n",
    "legend('topright',legend=names(table(mem_df$membership)),\n",
    "        col=palette,pch=19,ncol = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2866b2",
   "metadata": {},
   "source": [
    "Or by just by their community interactions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f24b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "idx <- base::match( V(clusters)$name, mem_df$names)\n",
    "\n",
    "cgg <- getCommunityGraph(clusters, mem_df$membership[idx])\n",
    "\n",
    "D0 = unname(degree(cgg))\n",
    "\n",
    "plot(cgg, vertex.size=sqrt(V(cgg)$size), vertex.cex = 0.8, vertex.color=round(log(D0)) + 1, layout=layout_with_kk, margin=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fb3614",
   "metadata": {},
   "source": [
    "## Transcription factor enrichment\n",
    "\n",
    "As we now have a ranked list with \"important\" genes, we can check if it's significantly enriched in transcription-factors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa058b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation_methods.fisher_exact_for_gene_lists import f_exact_test\n",
    "data = pd.read_csv(\"recons/GTEx_filtereded_with_genes_in_string.csv\")\n",
    "data = data.loc[data[\"recon_1\"] != 0]\n",
    "top_500 = data.iloc[0:500]\n",
    "f_exact_test(set(top_500[\"name\"]), set(data[\"name\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d970b6fc",
   "metadata": {},
   "source": [
    "To get an idea, about how the p-value will change over the ordered list of errors, we simply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f5e912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c009b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plist = []\n",
    "\n",
    "for genes in range(0, 1300):\n",
    "    top = data.iloc[0:genes]\n",
    "    results = f_exact_test(set(top[\"name\"]), set(data[\"name\"]))\n",
    "    plist.append(results[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680e9ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(plist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea18f57f",
   "metadata": {},
   "source": [
    "# Training with large files using a data generator\n",
    "\n",
    "If we want to work with large datasets, the `DataGenerator`-class implements everything needed to read from an h5 file, for training directly.\n",
    "With the generator you're able to shuffle the data between the batches, that can be useful in case one wants to iterate over the same data multiple times.\n",
    "The class has left some space for batch wise pre-processing.\n",
    "In this case we use a log-normalized and scaled version of the `EBPlusPlusAdjustPANCAN_IlluminaHiSeq_RNASeqV2.geneExp.tsv` dataset from `https://gdc.cancer.gov/about-data/publications/pancanatlas`.\n",
    "To use it with the data generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0a8704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from class_definitions import generic_VAE\n",
    "from class_definitions import data_handler\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# get information about the shape:\n",
    "data = h5py.File(\"./data/EBPlusPlusAdjustPANCAN.h5\").get(\"transposed_data\")\n",
    "# and define the generator\n",
    "data_train = data_handler.DataGenerator(dataset_name=\"transposed_data\", \n",
    "                                        filepath=\"./data/EBPlusPlusAdjustPANCAN.h5\",\n",
    "                                        # batch size hereby  \n",
    "                                        batch_size=10,\n",
    "                                        # shuffel the data\n",
    "                                        shuffle=True)\n",
    "input_dims = data[0].shape[0]\n",
    "\n",
    "# build a VAE:\n",
    "vae_build = generic_VAE.Builder(\n",
    "        input_dims,\n",
    "        # using a deeper model than before\n",
    "        [10000, 1000, 500],\n",
    "        [10000, 1000, 500],\n",
    "        100,\n",
    "        dropout_rate=.01)\n",
    "\n",
    "# make it a Model:\n",
    "vae_model = generic_VAE.VAE(vae_build)\n",
    "\n",
    "# complile it:\n",
    "vae_model.compile()\n",
    "\n",
    "# for the amount of batches:\n",
    "batch_size = int(np.floor(input_dims / 100))\n",
    "\n",
    "# vae_model.fit(data, batch_size=batch_size)\n",
    "vae_model.fit(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf19321",
   "metadata": {},
   "source": [
    "# Filters applied prior training\n",
    "\n",
    "In the introduction we simply searched for all genes, that were present in the STRINGdb database.\n",
    "This lead to a latent space, learned from the feature data.\n",
    "However this data was filtered prior to training, therefor we introduced a bias.\n",
    "\n",
    "Using this bias to focus on a particular group of genes can come in handy.\n",
    "A GO-term mapping a certain process for example is a great filter as we can specify gene products with it, that are known to be a part of it.\n",
    "\n",
    "For example counts the GO-term `GO:0006355` specifies the process of *regulation of DNA-templated transcription* and counts currently 21'789'333 annotations.\n",
    "The term `GO:0008134` specifies *transcription factor binding* and counts 96'125 annotations.\n",
    "\n",
    "## define a Filter\n",
    "\n",
    "We can get the names of genes (for *Homo sapiens*) annotated to these terms by using R again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5762472",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -o trans_assoc_genes\n",
    "library(GO.db)\n",
    "library(org.Hs.eg.db)\n",
    "\n",
    "# the ID's\n",
    "go_id = \"GO:0006355\"\n",
    "tf_bnd_id = \"GO:0008134\"\n",
    "\n",
    "# to get the genes annotated to the terms above\n",
    "allegs = get(go_id, org.Hs.egGO2ALLEGS)\n",
    "genes = unlist(mget(allegs,org.Hs.egSYMBOL))\n",
    "\n",
    "allegs = get(tf_bnd_id, org.Hs.egGO2ALLEGS)\n",
    "tf_bnd_genes = unlist(mget(allegs, org.Hs.egSYMBOL))\n",
    "\n",
    "trans_assoc_genes = c(tf_bnd_genes, genes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c59a38",
   "metadata": {},
   "source": [
    "To get arid of redundancies we'll simply make it a set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2ed310",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_assoc_genes = set(trans_assoc_genes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3bb72a",
   "metadata": {},
   "source": [
    "## How to use the Filter\n",
    "\n",
    "To apply this list of genes as a filter to the already used GTEx dataset we just:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86777ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_hdf(\"data/GTEx_Analysis_2017-06-05_v8_RNASeQCv1.1.9_gene_reads.h5\", key = \"data\")\n",
    "data = data.loc[data[\"Description\"].isin(trans_assoc_genes)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37194d1",
   "metadata": {},
   "source": [
    "## Model fitting\n",
    "\n",
    "This datset now can undergo some pre-processing, this time we will also remove genes that count less than 17383 reads per gene for all samples.\n",
    "And in the next step we use it to fit a VAE-model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4204e2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(data[data.iloc[:, 2:].sum(axis=1) < 17383].index)\n",
    "\n",
    "fdat = np.array(data)\n",
    "\n",
    "logdat = prep.log_norm(np.array(fdat)[:, 2:])\n",
    "\n",
    "sdat = prep.scale_by_sample(logdat)\n",
    "\n",
    "input_shape = sdat.T.shape[1]\n",
    "vae = gvae.Builder(input_shape=input_shape,\n",
    "                   encoder_shape=[186],\n",
    "                   decoder_shape=[186],\n",
    "                   latent_dims=18,\n",
    "                   dropout_rate=.001)\n",
    "\n",
    "vae = gvae.VAE(vae)\n",
    "\n",
    "# compile it:\n",
    "vae.compile()\n",
    "\n",
    "# and train it:\n",
    "vae.fit(sdat.T, epochs=100, workers=64, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da551ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "recons = vae.fwise_recon_error_tracker.result().numpy()\n",
    "recon_frame = pd.DataFrame({\"name\" : data[\"Description\"], \"recon_1\" : recons}).sort_values(by=\"recon_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cedcc28",
   "metadata": {},
   "source": [
    "## Further analysis\n",
    "\n",
    "Like we already did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b8889e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i recon_frame\n",
    "ref = recon_frame$name\n",
    "\n",
    "top_500 = recon_frame$name[1:501]\n",
    "\n",
    "# for cellular component\n",
    "ego = enrichGO(gene = top_500,\n",
    "               OrgDb = org.Hs.eg.db,\n",
    "               universe = ref,\n",
    "               ont = \"CC\",\n",
    "               keyType = \"SYMBOL\")\n",
    "barplot(ego)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa906365",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# for BP\n",
    "ego = enrichGO(gene = top_500,\n",
    "               OrgDb = org.Hs.eg.db,\n",
    "               universe = ref,\n",
    "               ont = \"BP\",\n",
    "               keyType = \"SYMBOL\")\n",
    "barplot(ego)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337acb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# for MF\n",
    "ego = enrichGO(gene = top_500,\n",
    "               OrgDb = org.Hs.eg.db,\n",
    "               universe = ref,\n",
    "               ont = \"MF\",\n",
    "               keyType = \"SYMBOL\")\n",
    "barplot(ego)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382d375c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library('MOMA')\n",
    "\n",
    "ekegg = enrichKEGG(gene = mapHugo(top_500),\n",
    "                   organism=\"hsa\")\n",
    "barplot(ekegg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d0df6c",
   "metadata": {},
   "source": [
    "For the computation of the p-values used in the fishers exact test, the defaulted list holds transcription factors from *Mus musculus* and *Homo sapiens*.\n",
    "If one want's to focus on different sets, simply hand them directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1a66e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfs_list = pd.read_csv(\"evaluation_methods/trrust_rawdata.human.tsv\", sep=\"\\t\")\n",
    "# remove redundancies:\n",
    "tfs = set(tfs_list[\"AATF\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2c62a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluation_methods.fisher_exact_for_gene_lists as ftest\n",
    "data = recon_frame.loc[recon_frame[\"recon_1\"] != 0]\n",
    "top_100 = recon_frame.iloc[0:100]\n",
    "ftest.f_exact_test(set(top_100[\"name\"]), set(recon_frame[\"name\"]), tfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4947636d",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_500 = recon_frame.iloc[0:500]\n",
    "ftest.f_exact_test(set(top_500[\"name\"]), set(recon_frame[\"name\"]), tfs)\n",
    "\n",
    "plist = []\n",
    "\n",
    "for genes in range(0, 1300):\n",
    "    top = recon_frame.iloc[0:genes]\n",
    "    results = ftest.f_exact_test(set(top[\"name\"]), set(recon_frame[\"name\"]), tfs)\n",
    "    plist.append(results[0][1])\n",
    "\n",
    "plt.plot(plist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6172a7f",
   "metadata": {},
   "source": [
    "As we're interested in the tf's, that have the lowest reconstruction error, we need to get an idea, where our list needs to be cut off:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e55734",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = plist.index(min(plist))\n",
    "top_of_the_pops = data[0:idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b38e54",
   "metadata": {},
   "source": [
    "To get some more knowledge regarding the function of these transcription factors, lets extract them:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc042a20",
   "metadata": {},
   "source": [
    "tfs_in_tops = tfs & set(top_of_the_pops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a3fb92",
   "metadata": {},
   "source": [
    "# Comparing two similar sets\n",
    "\n",
    "The data sets under `https://doi.org/10.1038/sdata.2018.61` supply us with data for cancer and normal tissue RNA-seq data.\n",
    "From there we downloaded breast cancer data of normal tissue from and also tumor tissue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e2cf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the cancer data:\n",
    "brca = pd.read_csv(\"/home/test/Downloads/brcarsemfpkmtcgat-tumor.txt.gz\", sep=\"\\t\")\n",
    "# the normal data from gtex:\n",
    "gtex = pd.read_csv(\"/home/test/Downloads/breastrsemfpkmgtex.txt.gz\", sep=\"\\t\")\n",
    "# and the normal data from tcga\n",
    "ntgc = pd.read_csv(\"/home/test/Downloads/brcarsemfpkmtcga.txt.gz\", sep=\"\\t\")\n",
    "# and join the normal data:\n",
    "normal = pd.concat([gtex, ntgc]).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa04a525",
   "metadata": {},
   "source": [
    "These now can be used for training VAEs, but at first the pre-processing takes place:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c8b6f4",
   "metadata": {},
   "source": [
    "## On the normal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f761742",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = normal.loc[normal[\"Hugo_Symbol\"].isin(trans_assoc_genes)]\n",
    "data = normal.drop(normal[normal.iloc[:, 2:].sum(axis=1) < 201].index)\n",
    "fdat = np.array(data)\n",
    "\n",
    "logdat = prep.log_norm(np.array(fdat)[:, 2:])\n",
    "\n",
    "sdat = prep.scale_by_sample(logdat)\n",
    "\n",
    "input_shape = sdat.T.shape[1]\n",
    "vae = gvae.Builder(input_shape=input_shape,\n",
    "                   encoder_shape=[500],\n",
    "                   decoder_shape=[500],\n",
    "                   latent_dims=100)\n",
    "\n",
    "vae = gvae.VAE(vae)\n",
    "\n",
    "# compile it:\n",
    "vae.compile()\n",
    "\n",
    "# and train it:\n",
    "vae.fit(sdat.T, epochs=10, batch_size=200 , workers=64, use_multiprocessing=True)\n",
    "# extract the recons:\n",
    "recons = vae.fwise_recon_error_tracker.result().numpy()\n",
    "recon_frame = pd.DataFrame({\"name\" : data[\"Hugo_Symbol\"], \"recon_1\" : recons}).sort_values(by=\"recon_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718e7ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "recon_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678beb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i recon_frame\n",
    "\n",
    "data = recon_frame %>% filter(recon_1 != 0)\n",
    "\n",
    "ref = data$name\n",
    "\n",
    "top_500 = data$name[1:501]\n",
    "\n",
    "# for cellular component\n",
    "ego = enrichGO(gene = top_500,\n",
    "               OrgDb = org.Hs.eg.db,\n",
    "               universe = ref,\n",
    "               ont = \"BP\",\n",
    "               keyType = \"SYMBOL\")\n",
    "\n",
    "barplot(ego)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e0506c",
   "metadata": {},
   "source": [
    "## Cancer-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c8cd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tumor = brca.loc[brca[\"Hugo_Symbol\"].isin(trans_assoc_genes)]\n",
    "data = tumor.fillna(0)\n",
    "data = tumor.drop(tumor[tumor.iloc[:, 2:].sum(axis=1) < 201].index)\n",
    "fdat = np.array(data)\n",
    "\n",
    "logdat = prep.log_norm(np.array(fdat)[:, 2:])\n",
    "\n",
    "sdat = prep.scale_by_sample(logdat)\n",
    "\n",
    "input_shape = sdat.T.shape[1]\n",
    "vae = gvae.Builder(input_shape=input_shape,\n",
    "                   encoder_shape=[500],\n",
    "                   decoder_shape=[500],\n",
    "                   latent_dims=100)\n",
    "\n",
    "vae = gvae.VAE(vae)\n",
    "\n",
    "# compile it:\n",
    "vae.compile()\n",
    "\n",
    "# and train it:\n",
    "vae.fit(sdat.T, epochs=40, workers=64, use_multiprocessing=True)\n",
    "# extract the recons:\n",
    "recons = vae.fwise_recon_error_tracker.result().numpy()\n",
    "recon_frame = pd.DataFrame({\"name\" : data[\"Hugo_Symbol\"], \"recon_1\" : recons}).sort_values(by=\"recon_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b59306",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i recon_frame\n",
    "\n",
    "data = recon_frame %>% filter(recon_1 != 0)\n",
    "\n",
    "ref = data$name\n",
    "\n",
    "top_500 = data$name[1:501]\n",
    "\n",
    "# for cellular component\n",
    "ego = enrichGO(gene = top_500,\n",
    "               OrgDb = org.Hs.eg.db,\n",
    "               universe = ref,\n",
    "               ont = \"BP\",\n",
    "               keyType = \"SYMBOL\")\n",
    "\n",
    "barplot(ego)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4645742e",
   "metadata": {},
   "source": [
    "# VAE-Architecture\n",
    "\n",
    "Most of the scenarios above were optimised with interactive processes.\n",
    "The architecture in the beginning was kept as shallow as possible.\n",
    "As a rough guideline one can reduce the amounts of neuronce at the beginning "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
