{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6207893-a2b8-41fb-b9b9-175f692dcbd5",
   "metadata": {},
   "source": [
    "# Intro to the usage of the genericVAE\n",
    "## Requirements\n",
    "\n",
    "This notebook uses python >= 3.7 with tensorflow version 2.12.0, numpy version 1.23.5, pandas version 2.0.0 and h5py 3.8.0.\n",
    "\n",
    "## Data\n",
    "\n",
    "The GTEx V8 study serves as a brilliant dataset for demonstration.\n",
    "At first we want to download a set that unifies all gene read counts, for different types of cells, in the study.\n",
    "\n",
    "The inclusion and exclusion criteria were specified as:\n",
    "\n",
    "1. 21 ≤ Age (years) ≤ 70\n",
    "2. 18.5 < Body Mass Index < 35\n",
    "3. Time between death and tissue collection less than 24 hours\n",
    "4. No whole blood transfusion within 48 hours prior to death\n",
    "5. No history of metastatic cancer\n",
    "6. No chemotherapy or radiation therapy within the 2 years prior to death\n",
    "7. Generally unselected for presence or absence of diseases or disorders, except for potentially communicable diseases that disqualify someone to donate organs or tissues would also be disqualifying for GTEx.\n",
    "\n",
    "This data can be downloaded from the following URL: `https://storage.googleapis.com/gtex_analysis_v8/rna_seq_data/GTEx_Analysis_2017-06-05_v8_RNASeQCv1.1.9_gene_reads.gct.gz` .\n",
    "After the download, move it to a folder `genericVAE/data` where you'll place all other data, too.\n",
    "To read the data a helping function is provided:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4eb9b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import reading\n",
    "data = reading.read_gct_from_GTEx(\"data/GTEx_Analysis_2017-06-05_v8_RNASeQCv1.1.9_gene_reads.gct.gz\")\n",
    "# data.info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11b2cae",
   "metadata": {},
   "source": [
    "We see, that there is rows specifying genes (by entrez-id and the HUGO-symbol) and columns specifying samples.\n",
    "As the dataset is quite a chunk, for further analysis we want to safe this data in format, that's faster accessible, for us that's h5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30eb10d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_hdf(\"data/GTEx_Analysis_2017-06-05_v8_RNASeQCv1.1.9_gene_reads.h5\", key=\"data\")\n",
    "del data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df3954c",
   "metadata": {},
   "source": [
    "Next we want to filter the dataset for genes, that are relevant for our current scope.\n",
    "For the beginning, lets focus on genes, that are part of the STRING database.\n",
    "For this purpose there is a list provided, named `h_S_string.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2b6d9cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "25a4b07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_hdf(\"data/GTEx_Analysis_2017-06-05_v8_RNASeQCv1.1.9_gene_reads.h5\", key = \"data\")\n",
    "in_string = pd.read_csv(\"h_S_string.txt\", sep=\"\\t\")[\"preferred_name\"]\n",
    "# filter the frame\n",
    "fdat = data.loc[data[\"Description\"].isin(in_string)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8b6328",
   "metadata": {},
   "source": [
    "Now we have a subset of 17383 samples reduced to 18647 gene reads per sample.\n",
    "Prior to training there needs to be done some pre-processing.\n",
    "Experimentation has shown, that log-transformation and scaling is enough.\n",
    "For the sake of performance we will switch to `numpy` and later reconstruct the data-frame structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fba55a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import helpers.data_preprocessing as prep\n",
    "\n",
    "# for the log-transformation\n",
    "logdat = prep.log_norm(np.array(fdat)[:, 2:])\n",
    "# for the scaling, we scale sample-wise, here we add 1 to avoid zero-division:\n",
    "sdat = prep.scale_by_sample(logdat+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5470f09",
   "metadata": {},
   "source": [
    "## Build and fit a model\n",
    "\n",
    "Now we can build a variational autoencoder (VAE) model to be trained with the data we just prepared.\n",
    "At first we specify the parameters the model will be based on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa5f1553",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-17 13:36:18.764596: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-17 13:36:18.797184: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-17 13:36:18.797927: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-17 13:36:19.484279: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import class_definitions.generic_VAE as gvae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34898389",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-17 13:36:20.066104: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "# as we train sample-wise, we have to transpose the dataframe\n",
    "input_shape = sdat.T.shape[1]\n",
    "\n",
    "vae = gvae.Builder(input_shape=input_shape,\n",
    "                   encoder_shape=[1000],\n",
    "                   decoder_shape=[1000],\n",
    "                   latent_dims=50)\n",
    "\n",
    "vae = gvae.VAE(vae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fa72e4",
   "metadata": {},
   "source": [
    "Next we compile it, in concordance with the `keras` functional API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a0375d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d588ce1",
   "metadata": {},
   "source": [
    "Before we can start training, lets find a batch-size assuming we want to train in 100 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3fb969e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = sdat.shape[1]//100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaa0699",
   "metadata": {},
   "source": [
    "And fit it to our pre-processed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "11627ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "101/101 [==============================] - 24s 238ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 2/10\n",
      "101/101 [==============================] - 24s 236ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 3/10\n",
      "101/101 [==============================] - 24s 236ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 4/10\n",
      "101/101 [==============================] - 24s 235ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 5/10\n",
      "101/101 [==============================] - 23s 232ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 6/10\n",
      "101/101 [==============================] - 23s 231ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 7/10\n",
      "101/101 [==============================] - 23s 231ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 8/10\n",
      "101/101 [==============================] - 23s 232ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 9/10\n",
      "101/101 [==============================] - 23s 231ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
      "Epoch 10/10\n",
      "101/101 [==============================] - 23s 231ms/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8921c33580>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.fit(sdat.T, epochs=10, batch_size= batchsize,workers=64, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85048b5a",
   "metadata": {},
   "source": [
    "In this case we just trained once with each sample.\n",
    "So far no training best practice was established.\n",
    "\n",
    "The difference here to usual cases of generative frameworks is, that the generative process is not the thing we aim to optimize, here the aim is to produce a representative latent space.\n",
    "\n",
    "This difference could also allow the user to iterate several times over the same data and cause soft overfitting as we don't aim to create an intelligent model.\n",
    "In case of using such a mechanism the introduction of drop-outs however is a good idea to still suppress overfitting effects getting too large.\n",
    "\n",
    "In the end, the user has to set the hyper parameters in a way, that seems appropriate for the underlying case.\n",
    "\n",
    "After fitting of the model one want's to extract the reconstruction errors.\n",
    "These reconstruction errors are the basis for the further analysis.\n",
    "The variational autoencoder reduces the input over our hidden layer towards\n",
    "a representation of only 18 dimensions.\n",
    "Therefor the system is ought to train itself in a way, that allows latent conservation of features that help to reduce this error.\n",
    "Some features will contribute more to this latent representation, some contribute less.\n",
    "Based on the contribution of a feature to the latent representation it's reconstruction error\n",
    "will be smaller (high contribution) or greater (less contribution).\n",
    "For this reason we'll remap the gene-names to the reconstruction errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9fcddd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to extract reconstruction-errors we simply access the vae-obj\n",
    "recons = vae.fwise_recon_error_tracker.result().numpy()\n",
    "recon_frame = pd.DataFrame({\"name\" : fdat[\"Description\"], \"recon_1\" : recons})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b6f10ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>recon_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ENSG00000186092.4</th>\n",
       "      <td>OR4F5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000278566.1</th>\n",
       "      <td>OR4F29</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000273547.1</th>\n",
       "      <td>OR4F16</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000187634.11</th>\n",
       "      <td>SAMD11</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000188976.10</th>\n",
       "      <td>NOC2L</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000212907.2</th>\n",
       "      <td>MT-ND4L</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000198886.2</th>\n",
       "      <td>MT-ND4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000198786.2</th>\n",
       "      <td>MT-ND5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000198695.2</th>\n",
       "      <td>MT-ND6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000198727.2</th>\n",
       "      <td>MT-CYB</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18647 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       name  recon_1\n",
       "Name                                \n",
       "ENSG00000186092.4     OR4F5      NaN\n",
       "ENSG00000278566.1    OR4F29      NaN\n",
       "ENSG00000273547.1    OR4F16      NaN\n",
       "ENSG00000187634.11   SAMD11      NaN\n",
       "ENSG00000188976.10    NOC2L      NaN\n",
       "...                     ...      ...\n",
       "ENSG00000212907.2   MT-ND4L      NaN\n",
       "ENSG00000198886.2    MT-ND4      NaN\n",
       "ENSG00000198786.2    MT-ND5      NaN\n",
       "ENSG00000198695.2    MT-ND6      NaN\n",
       "ENSG00000198727.2    MT-CYB      NaN\n",
       "\n",
       "[18647 rows x 2 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recon_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196ef56a",
   "metadata": {},
   "source": [
    "## Analysis of the reconstruction error list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da50c13",
   "metadata": {},
   "source": [
    "To dig a little deeper we use R.\n",
    "To use R inside of this notebook we use the ipython rpy2 extension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e5657980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rpy2.ipython extension is already loaded. To reload it, use:\n",
      "  %reload_ext rpy2.ipython\n"
     ]
    }
   ],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a4592d",
   "metadata": {},
   "source": [
    "To invoke R in an IPythbon cell simply use the magic function `%%R`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bf108d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Hello, it's R  4 2.3  printing this\"\n"
     ]
    }
   ],
   "source": [
    "%%R \n",
    "string = paste(\"Hello, it's R \", R.Version()$major, R.Version()$minor, \" printing this\")\n",
    "print(string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f147786f",
   "metadata": {},
   "source": [
    "### Enrichment analysis\n",
    "\n",
    "At first let's have a look the top-200 genes with the lowest reconstruction error and check the enriched terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1dab5529",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library('tidyverse')\n",
    "library('clusterProfiler')\n",
    "library('org.Hs.eg.db')\n",
    "library('tidyverse')\n",
    "library('STRINGdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "82e7c80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data.frame':\t18647 obs. of  2 variables:\n",
      " $ name   : chr  \"OR4F5\" \"OR4F29\" \"OR4F16\" \"SAMD11\" ...\n",
      " $ recon_1: num  NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ...\n"
     ]
    }
   ],
   "source": [
    "%%R -i recon_frame\n",
    "str(recon_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ddf17fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: --> No gene can be mapped....\n",
      "\n",
      "R[write to console]: --> Expected input gene ID: TAF10,ERCC8,SIN3B,EPC1,ACTL6B,ZNF217\n",
      "\n",
      "R[write to console]: --> return NULL...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NULL\n"
     ]
    }
   ],
   "source": [
    "%%R -i recon_frame\n",
    "\n",
    "data = recon_frame %>% filter(recon_1 != 0)\n",
    "\n",
    "ref = data$name\n",
    "\n",
    "top_500 = data$name[1:501]\n",
    "\n",
    "# for cellular component\n",
    "ego = enrichGO(gene = top_500,\n",
    "               OrgDb = org.Hs.eg.db,\n",
    "               universe = ref,\n",
    "               ont = \"CC\",\n",
    "               keyType = \"SYMBOL\")\n",
    "\n",
    "barplot(ego)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a840b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# and molecular function\n",
    "ego = enrichGO(gene = top_500,\n",
    "               OrgDb = org.Hs.eg.db,\n",
    "               universe = ref,\n",
    "               ont = \"MF\",\n",
    "               keyType = \"SYMBOL\")\n",
    "\n",
    "barplot(ego)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c061778",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# and biological process:\n",
    "ego = enrichGO(gene = top_500,\n",
    "               OrgDb = org.Hs.eg.db,\n",
    "               universe = ref,\n",
    "               ont = \"BP\",\n",
    "               keyType = \"SYMBOL\")\n",
    "\n",
    "barplot(ego)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e93ef5",
   "metadata": {},
   "source": [
    "### Network analysis\n",
    "\n",
    "We can also look closer at the genes by plotting them as PPI network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea69786",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "string_db = STRINGdb$new(species=9606,\n",
    "                         score_threshold=200,\n",
    "                         input_directory=\"\")\n",
    "\n",
    "top_500 = data.frame(\"gene\"=top_500)\n",
    "\n",
    "top_500_mapped = string_db$map(top_500, \"gene\", removeUnmappedRows = T)\n",
    "string_db$plot_network(top_500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603d912c",
   "metadata": {},
   "source": [
    "Seems like we have a few heavy interconnected regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04029377",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "ntwrk <- string_db$get_subnetwork(top_500_mapped$STRING_id)\n",
    "\n",
    "library('igraph')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275178d2",
   "metadata": {},
   "source": [
    "With igraph, let us have a look at some measures we can easily compute on the extracted network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165ec24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# \n",
    "order(betweenness(ntwrk))\n",
    "mean_distance(ntwrk)\n",
    "clique.number(ntwrk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402d0091",
   "metadata": {},
   "source": [
    "For a further analysis let's switch to the package `BioNAR`.\n",
    "Let's have a look at how well our subnetwork follows the power-law:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5aa70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library('BioNAR')\n",
    "clusters <- calcAllClustering(ntwrk)\n",
    "pFit <- fitDegree( as.vector(igraph::degree(graph=clusters)),threads=1, Nsim=5,\n",
    "\n",
    "                    plot=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06511607",
   "metadata": {},
   "source": [
    "The package also allows us to identify clusters in the extracted list.\n",
    "These clusters then can get extracted and be plotted clusterwise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57f8a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "alg = \"louvain\"\n",
    "library(\"randomcoloR\")\n",
    "\n",
    "clsuters <- calcCentrality(ntwrk)\n",
    "getCentralityMatrix(clusters)\n",
    "clusters <- calcClustering(clsuters, alg)\n",
    "summary(clusters)\n",
    "V(clusters)$louvain\n",
    "\n",
    "mem_df <- data.frame(names=V(clusters)$name,membership=as.numeric(V(clusters)$louvain))\n",
    "\n",
    "palette <- distinctColorPalette(max(as.numeric(mem_df$membership)))\n",
    "\n",
    "lay <- layoutByCluster(clusters, mem_df,layout = layout_nicely)\n",
    "\n",
    "plot(clusters,vertex.size=3,layout=lay,\n",
    "        vertex.label=NA,\n",
    "        vertex.color=palette[as.numeric(mem_df$membership)],\n",
    "        edge.color='grey95')\n",
    "\n",
    "legend('topright',legend=names(table(mem_df$membership)),\n",
    "        col=palette,pch=19,ncol = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2866b2",
   "metadata": {},
   "source": [
    "Or by just by their community interactions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f24b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "idx <- base::match( V(clusters)$name, mem_df$names)\n",
    "\n",
    "cgg <- getCommunityGraph(clusters, mem_df$membership[idx])\n",
    "\n",
    "D0 = unname(degree(cgg))\n",
    "\n",
    "plot(cgg, vertex.size=sqrt(V(cgg)$size), vertex.cex = 0.8, vertex.color=round(log(D0)) + 1, layout=layout_with_kk, margin=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fb3614",
   "metadata": {},
   "source": [
    "## Transcription factor enrichment\n",
    "\n",
    "As we now have a ranked list with \"important\" genes, we can check if it's significantly enriched in transcription-factors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa058b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation_methods.fisher_exact_for_gene_lists import f_exact_test\n",
    "data = pd.read_csv(\"recons/GTEx_filtereded_with_genes_in_string.csv\")\n",
    "data = data.loc[data[\"recon_1\"] != 0]\n",
    "top_500 = data.iloc[0:500]\n",
    "f_exact_test(set(top_500[\"name\"]), set(data[\"name\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d970b6fc",
   "metadata": {},
   "source": [
    "To get an idea, about how the p-value will change over the ordered list of errors, we simply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f5e912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c009b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plist = []\n",
    "\n",
    "for genes in range(0, 1300):\n",
    "    top = data.iloc[0:genes]\n",
    "    results = f_exact_test(set(top[\"name\"]), set(data[\"name\"]))\n",
    "    plist.append(results[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680e9ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(plist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea18f57f",
   "metadata": {},
   "source": [
    "# Training with large files using a data generator\n",
    "\n",
    "If we want to work with large datasets, the `DataGenerator`-class implements everything needed to read from an h5 file, for training directly.\n",
    "This can come in handy for example when the RAM of the used computer is too small to load a whole set at once.\n",
    "With the generator you're able to shuffle the data between the batches, that can be useful in case one wants to iterate over the same data multiple times.\n",
    "The class has left some space for batch wise pre-processing.\n",
    "In this case we use a log-normalized and scaled version of the `EBPlusPlusAdjustPANCAN_IlluminaHiSeq_RNASeqV2.geneExp.tsv` dataset from `https://gdc.cancer.gov/about-data/publications/pancanatlas`.\n",
    "To use it with the data generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0a8704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from class_definitions import generic_VAE\n",
    "from class_definitions import data_handler\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# get information about the shape:\n",
    "data = h5py.File(\"./data/EBPlusPlusAdjustPANCAN.h5\").get(\"transposed_data\")\n",
    "# and define the generator\n",
    "data_train = data_handler.DataGenerator(dataset_name=\"transposed_data\", \n",
    "                                        filepath=\"./data/EBPlusPlusAdjustPANCAN.h5\",\n",
    "                                        # batch size hereby  \n",
    "                                        batch_size=10,\n",
    "                                        # shuffel the data\n",
    "                                        shuffle=True)\n",
    "input_dims = data[0].shape[0]\n",
    "\n",
    "# build a VAE:\n",
    "vae_build = generic_VAE.Builder(\n",
    "        input_dims,\n",
    "        # using a deeper model than before\n",
    "        [10000, 1000, 500],\n",
    "        [10000, 1000, 500],\n",
    "        100,\n",
    "        dropout_rate=.01)\n",
    "\n",
    "# make it a Model:\n",
    "vae_model = generic_VAE.VAE(vae_build)\n",
    "\n",
    "# complile it:\n",
    "vae_model.compile()\n",
    "\n",
    "# for the amount of batches:\n",
    "batch_size = int(np.floor(input_dims / 100))\n",
    "\n",
    "# vae_model.fit(data, batch_size=batch_size)\n",
    "vae_model.fit(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf19321",
   "metadata": {},
   "source": [
    "# Filters applied prior training\n",
    "\n",
    "In the introduction we simply searched for all genes, that were present in the STRINGdb database.\n",
    "This lead to a latent space, learned from the feature data.\n",
    "However this data was filtered prior to training, therefor we introduced a bias.\n",
    "\n",
    "Using this bias to focus on a particular group of genes can come in handy.\n",
    "A GO-term mapping a certain process for example is a great filter as we can specify gene products with it, that are known to be a part of it.\n",
    "\n",
    "For example counts the GO-term `GO:0006355` specifies the process of *regulation of DNA-templated transcription* and counts currently 21'789'333 annotations.\n",
    "The term `GO:0008134` specifies *transcription factor binding* and counts 96'125 annotations.\n",
    "\n",
    "## define a Filter\n",
    "\n",
    "We can get the names of genes (for *Homo sapiens*) annotated to these terms by using R again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5762472",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -o trans_assoc_genes\n",
    "library(GO.db)\n",
    "library(org.Hs.eg.db)\n",
    "\n",
    "# the ID's\n",
    "go_id = \"GO:0006355\"\n",
    "tf_bnd_id = \"GO:0008134\"\n",
    "\n",
    "# to get the genes annotated to the terms above\n",
    "allegs = get(go_id, org.Hs.egGO2ALLEGS)\n",
    "genes = unlist(mget(allegs,org.Hs.egSYMBOL))\n",
    "\n",
    "allegs = get(tf_bnd_id, org.Hs.egGO2ALLEGS)\n",
    "tf_bnd_genes = unlist(mget(allegs, org.Hs.egSYMBOL))\n",
    "\n",
    "trans_assoc_genes = c(tf_bnd_genes, genes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c59a38",
   "metadata": {},
   "source": [
    "To get arid of redundancies we'll simply make it a set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2ed310",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_assoc_genes = set(trans_assoc_genes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3bb72a",
   "metadata": {},
   "source": [
    "## How to use the Filter\n",
    "\n",
    "To apply this list of genes as a filter to the already used GTEx dataset we just:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86777ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_hdf(\"data/GTEx_Analysis_2017-06-05_v8_RNASeQCv1.1.9_gene_reads.h5\", key = \"data\")\n",
    "data = data.loc[data[\"Description\"].isin(trans_assoc_genes)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37194d1",
   "metadata": {},
   "source": [
    "## Model fitting\n",
    "\n",
    "This datset now can undergo some pre-processing, this time we will also remove genes that count less than 17383 reads per gene for all samples.\n",
    "And in the next step we use it to fit a VAE-model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4204e2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(data[data.iloc[:, 2:].sum(axis=1) < 17383].index)\n",
    "\n",
    "fdat = np.array(data)\n",
    "\n",
    "logdat = prep.log_norm(np.array(fdat)[:, 2:])\n",
    "\n",
    "sdat = prep.scale_by_sample(logdat)\n",
    "\n",
    "input_shape = sdat.T.shape[1]\n",
    "vae = gvae.Builder(input_shape=input_shape,\n",
    "                   encoder_shape=[186],\n",
    "                   decoder_shape=[186],\n",
    "                   latent_dims=18,\n",
    "                   dropout_rate=.001)\n",
    "\n",
    "vae = gvae.VAE(vae)\n",
    "\n",
    "# compile it:\n",
    "vae.compile()\n",
    "\n",
    "# and train it:\n",
    "vae.fit(sdat.T, epochs=100, workers=64, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da551ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "recons = vae.fwise_recon_error_tracker.result().numpy()\n",
    "recon_frame = pd.DataFrame({\"name\" : data[\"Description\"], \"recon_1\" : recons}).sort_values(by=\"recon_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cedcc28",
   "metadata": {},
   "source": [
    "## Further analysis\n",
    "\n",
    "Like we already did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b8889e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i recon_frame\n",
    "ref = recon_frame$name\n",
    "\n",
    "top_500 = recon_frame$name[1:501]\n",
    "\n",
    "# for cellular component\n",
    "ego = enrichGO(gene = top_500,\n",
    "               OrgDb = org.Hs.eg.db,\n",
    "               universe = ref,\n",
    "               ont = \"CC\",\n",
    "               keyType = \"SYMBOL\")\n",
    "barplot(ego)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa906365",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# for BP\n",
    "ego = enrichGO(gene = top_500,\n",
    "               OrgDb = org.Hs.eg.db,\n",
    "               universe = ref,\n",
    "               ont = \"BP\",\n",
    "               keyType = \"SYMBOL\")\n",
    "barplot(ego)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337acb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# for MF\n",
    "ego = enrichGO(gene = top_500,\n",
    "               OrgDb = org.Hs.eg.db,\n",
    "               universe = ref,\n",
    "               ont = \"MF\",\n",
    "               keyType = \"SYMBOL\")\n",
    "barplot(ego)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382d375c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library('MOMA')\n",
    "\n",
    "ekegg = enrichKEGG(gene = mapHugo(top_500),\n",
    "                   organism=\"hsa\")\n",
    "barplot(ekegg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d0df6c",
   "metadata": {},
   "source": [
    "For the computation of the p-values used in the fishers exact test, the defaulted list holds transcription factors from *Mus musculus* and *Homo sapiens*.\n",
    "If one want's to focus on different sets, simply hand them directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1a66e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfs_list = pd.read_csv(\"evaluation_methods/trrust_rawdata.human.tsv\", sep=\"\\t\")\n",
    "# remove redundancies:\n",
    "tfs = set(tfs_list[\"AATF\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2c62a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluation_methods.fisher_exact_for_gene_lists as ftest\n",
    "data = recon_frame.loc[recon_frame[\"recon_1\"] != 0]\n",
    "top_100 = recon_frame.iloc[0:100]\n",
    "ftest.f_exact_test(set(top_100[\"name\"]), set(recon_frame[\"name\"]), tfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4947636d",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_500 = recon_frame.iloc[0:500]\n",
    "ftest.f_exact_test(set(top_500[\"name\"]), set(recon_frame[\"name\"]), tfs)\n",
    "\n",
    "plist = []\n",
    "\n",
    "for genes in range(0, 1300):\n",
    "    top = recon_frame.iloc[0:genes]\n",
    "    results = ftest.f_exact_test(set(top[\"name\"]), set(recon_frame[\"name\"]), tfs)\n",
    "    plist.append(results[0][1])\n",
    "\n",
    "plt.plot(plist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6172a7f",
   "metadata": {},
   "source": [
    "As we're interested in the tf's, that have the lowest reconstruction error, we need to get an idea, where our list needs to be cut off:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e55734",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = plist.index(min(plist))\n",
    "top_of_the_pops = data[0:idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b38e54",
   "metadata": {},
   "source": [
    "To get some more knowledge regarding the function of these transcription factors, lets extract them:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc042a20",
   "metadata": {},
   "source": [
    "tfs_in_tops = tfs & set(top_of_the_pops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a3fb92",
   "metadata": {},
   "source": [
    "# Comparing two similar sets\n",
    "\n",
    "The data sets under `https://doi.org/10.1038/sdata.2018.61` supply us with data for cancer and normal tissue RNA-seq data.\n",
    "From there we downloaded breast cancer data of normal tissue from and also tumor tissue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e2cf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the cancer data:\n",
    "brca = pd.read_csv(\"/home/test/Downloads/brcarsemfpkmtcgat-tumor.txt.gz\", sep=\"\\t\")\n",
    "# the normal data from gtex:\n",
    "gtex = pd.read_csv(\"/home/test/Downloads/breastrsemfpkmgtex.txt.gz\", sep=\"\\t\")\n",
    "# and the normal data from tcga\n",
    "ntgc = pd.read_csv(\"/home/test/Downloads/brcarsemfpkmtcga.txt.gz\", sep=\"\\t\")\n",
    "# and join the normal data:\n",
    "normal = pd.concat([gtex, ntgc]).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa04a525",
   "metadata": {},
   "source": [
    "These now can be used for training VAEs, but at first the pre-processing takes place:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c8b6f4",
   "metadata": {},
   "source": [
    "## On the normal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f761742",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = normal.loc[normal[\"Hugo_Symbol\"].isin(trans_assoc_genes)]\n",
    "data = normal.drop(normal[normal.iloc[:, 2:].sum(axis=1) < 201].index)\n",
    "fdat = np.array(data)\n",
    "\n",
    "logdat = prep.log_norm(np.array(fdat)[:, 2:])\n",
    "\n",
    "sdat = prep.scale_by_sample(logdat)\n",
    "\n",
    "input_shape = sdat.T.shape[1]\n",
    "vae = gvae.Builder(input_shape=input_shape,\n",
    "                   encoder_shape=[500],\n",
    "                   decoder_shape=[500],\n",
    "                   latent_dims=100)\n",
    "\n",
    "vae = gvae.VAE(vae)\n",
    "\n",
    "# compile it:\n",
    "vae.compile()\n",
    "\n",
    "# and train it:\n",
    "vae.fit(sdat.T, epochs=10, batch_size=200 , workers=64, use_multiprocessing=True)\n",
    "# extract the recons:\n",
    "recons = vae.fwise_recon_error_tracker.result().numpy()\n",
    "recon_frame = pd.DataFrame({\"name\" : data[\"Hugo_Symbol\"], \"recon_1\" : recons}).sort_values(by=\"recon_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718e7ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "recon_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678beb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i recon_frame\n",
    "\n",
    "data = recon_frame %>% filter(recon_1 != 0)\n",
    "\n",
    "ref = data$name\n",
    "\n",
    "top_500 = data$name[1:501]\n",
    "\n",
    "# for cellular component\n",
    "ego = enrichGO(gene = top_500,\n",
    "               OrgDb = org.Hs.eg.db,\n",
    "               universe = ref,\n",
    "               ont = \"BP\",\n",
    "               keyType = \"SYMBOL\")\n",
    "\n",
    "barplot(ego)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e0506c",
   "metadata": {},
   "source": [
    "## Cancer-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c8cd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tumor = brca.loc[brca[\"Hugo_Symbol\"].isin(trans_assoc_genes)]\n",
    "data = tumor.fillna(0)\n",
    "data = tumor.drop(tumor[tumor.iloc[:, 2:].sum(axis=1) < 201].index)\n",
    "fdat = np.array(data)\n",
    "\n",
    "logdat = prep.log_norm(np.array(fdat)[:, 2:])\n",
    "\n",
    "sdat = prep.scale_by_sample(logdat)\n",
    "\n",
    "input_shape = sdat.T.shape[1]\n",
    "vae = gvae.Builder(input_shape=input_shape,\n",
    "                   encoder_shape=[500],\n",
    "                   decoder_shape=[500],\n",
    "                   latent_dims=100)\n",
    "\n",
    "vae = gvae.VAE(vae)\n",
    "\n",
    "# compile it:\n",
    "vae.compile()\n",
    "\n",
    "# and train it:\n",
    "vae.fit(sdat.T, epochs=40, workers=64, use_multiprocessing=True)\n",
    "# extract the recons:\n",
    "recons = vae.fwise_recon_error_tracker.result().numpy()\n",
    "recon_frame = pd.DataFrame({\"name\" : data[\"Hugo_Symbol\"], \"recon_1\" : recons}).sort_values(by=\"recon_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b59306",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i recon_frame\n",
    "\n",
    "data = recon_frame %>% filter(recon_1 != 0)\n",
    "\n",
    "ref = data$name\n",
    "\n",
    "top_500 = data$name[1:501]\n",
    "\n",
    "# for cellular component\n",
    "ego = enrichGO(gene = top_500,\n",
    "               OrgDb = org.Hs.eg.db,\n",
    "               universe = ref,\n",
    "               ont = \"BP\",\n",
    "               keyType = \"SYMBOL\")\n",
    "\n",
    "barplot(ego)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e90bace",
   "metadata": {},
   "source": [
    "# VAE-Architecture\n",
    "\n",
    "Most of the scenarios above were optimized with interactive processes.\n",
    "The architecture in the beginning was kept as shallow as possible.\n",
    "As a rough guideline one can reduce the amounts of neurons at the beginning with 2-3 times the power of ten and create a latent layer of a size between 100 and 10 neurons.\n",
    "If this does not lead to a sufficient outcome under a training considering all sample-sets once, the VAE can usually achieve better results by adding another hidden layer.\n",
    "It's important to notice, that this can have an impact to the final ranking of genes by their reconstruction errors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
